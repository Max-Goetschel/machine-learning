{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python for Machine Learning\n",
    "\n",
    "### *Session \\#5*\n",
    "\n",
    "\n",
    "### Helpful shortcuts\n",
    "---\n",
    "\n",
    "**SHIFT** + **ENTER** ----> Execute Cell\n",
    "\n",
    "**UP/DOWN ARROWS** --> Move cursor between cells (then ENTER to start typing)\n",
    "\n",
    "**TAB** ----> See autocomplete options\n",
    "\n",
    "**ESC** then **b** ----> Create Cell \n",
    "\n",
    "**ESC** then **dd** ----> Delete Cell\n",
    "\n",
    "**\\[python expression\\]?** ---> Explanation of that Python expression\n",
    "\n",
    "**ESC** then **m** then __ENTER__ ----> Switch to Markdown mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warm Ups\n",
    "\n",
    "*Type the given code into the cell below*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt \n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "from yellowbrick.classifier import ClassPredictionError\n",
    "from yellowbrick.model_selection import ValidationCurve\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv(\"diamonds.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define a function** `knn` **that takes two parameters: a dataframe** `data` **and a row** `diamond`\n",
    "\n",
    "**Make the function add a** `distance` **column to** `data` **which is the difference between the** `carat` **column in each row and the** `carat` **column of** `diamond`\n",
    "\n",
    "*Note: for simplicity, we are going to skip using `train_test_split` for this example*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Change the** `distance` **column to be the squared difference instead, so that it's never negative. Now sort** `data` **according to** `distance` **using the** `sort_values()` **method.**\n",
    "\n",
    "**Return the top five closest rows with** `.head()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Change the function to return the mean of the** `carat` **values, instead of the rows themselves**\n",
    "\n",
    "**Add a parameter** `k` **to your function which controls how many nearest neighbors you use.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Change** `distance` **to be computed based on the** `table` **and** `depth` **columns as well**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create KNN Classifier**: `model = KNeighborsClassifier(n_neighbors=6)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "---\n",
    "\n",
    "**1. Divide the dataset into a training set and test set, then train a KNeighborsClassifier on it.**\n",
    "\n",
    "*Note: In this exercise the `label` column is what we are trying to predict*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Use a validation curve to choose an optimal** `n_neighbors`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Use the** `jupyter_drawing_pad` **widget to draw a digit**\n",
    "\n",
    "*Hint: Draw slowly to get more data points*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jupyter_drawing_pad as jd\n",
    "widget = jd.CustomBox()\n",
    "widget.drawing_pad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Run the following code, which convert the drawing into simple numerical form and plots it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(widget):\n",
    "    x_data = widget.drawing_pad.data[0]\n",
    "    y_data = widget.drawing_pad.data[1]\n",
    "\n",
    "    x_axis = np.linspace(min(x_data), max(x_data), 8)\n",
    "    y_axis = np.linspace(min(y_data), max(y_data), 8)\n",
    "\n",
    "    x_interval = x_axis[1] - x_axis[0]\n",
    "    y_interval = y_axis[1] - y_axis[0]\n",
    "\n",
    "    data = np.array(list(zip(x_data,y_data)), dtype=[('x', '<f8'), ('y', '<f8')])\n",
    "    totals = np.zeros((8,8))\n",
    "\n",
    "    for x_num, x in enumerate(x_axis):\n",
    "        for y_num, y in enumerate(y_axis):\n",
    "            count = len(data[(data['x'] > x) & (data['x'] < x + x_interval)&(data['y'] > y) & (data['y'] < y + y_interval) ])\n",
    "            totals[x_num,y_num] = count*5 if count < 10 else 50\n",
    "    return np.rot90(totals).reshape(1, -1)\n",
    "\n",
    "num = extract_data(widget)\n",
    "plt.imshow(num.reshape(8,8), cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Use** `model.predict()` **to see if the model got your digit right. If not, try using** `model.predict_proba()` **to see how close it was**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warm Ups\n",
    "\n",
    "*Type the given code into the cell below*\n",
    "\n",
    "---\n",
    "**Create a decision tree:** `model = DecisionTreeClassifier(max_depth=2)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "---\n",
    "\n",
    "**1. Load the dataset and divide it into a training set and test set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"cars.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Create a pipeline with** `OneHotEncoder` **and a** `DecisionTreeClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Set the random_state parameter on** `train_test_split` **and adjust the max_depth parameter to optimize the model. You can use a ValidationCurve to find the best value.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Print out the resulting decision tree using** `plot_tree()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warm Ups\n",
    "\n",
    "*Type the given code into the cell below*\n",
    "\n",
    "---\n",
    "\n",
    "**Create a random forest**: `model = RandomForestClassifier(n_estimators=100)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "---\n",
    "\n",
    "**1. Divide the dataset into a training set and test set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Create a pipeline with a OneHotEncoder and a RandomForestClassifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Set the random_state parameter on** `train_test_split` **and adjust the n_estimators parameter to optimize the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
